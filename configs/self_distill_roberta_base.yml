key:
  k_fold: 5
  rng_seed: 0
  model_type: "self-distill"
  model_name: "roberta-base"
  embedding_method: "weight-pool"
  checkpoint_dir: "./checkpoints/self_distill_roberta_base"
  pretrained_dir: "./checkpoints/pretrained/roberta_base"

  train_method: "vanilla"
  scheduler_method: 'cosine'
  batch_size: 16
  max_epoch: 15
  weight_decay: 0.01
  optimizer_name: "AdamW"
  lr: 5e-5
  warmup_steps: 0
  embedding_layer_start: 9
  gradient_accumulation_steps: 1
  warmup_proportion: 0.1
  hidden_dropout_prob: 0.0
  attention_probs_dropout_prob: 0.0

  dataset_properties:
    -
      name: commonlit
      task: "reg"
      train_data_path: "../data/train.csv"
      text_column: 'text'
      loss_name: 'dist'
      evaluator: 'RMSE'
      log_every: 10
      eval_every: 10
      branches:
        - label
        - standard_error
      num_classes:
        - 1
        - 1
      last_layers:
        - linear
        - linear
      distill_start_epoch: 1
      predict_method: 'knn'
      knn_k: 5
