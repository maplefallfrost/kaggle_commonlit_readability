import torch
import torch.nn as nn
import os

from transformers import AutoConfig, AutoModelForMaskedLM
from models.util import create_last_layers
from modules.weighted_layer_pooling import WeightedLayerPooling
from loss import LossWrapper
from util import get_class_to_score, prob_to_mean

class Roberta(nn.Module):
    def __init__(self, config):
        """
        config: argparse.Namespace.
        """
        super().__init__()
        # just for getting the model device.
        self.dummy_param = nn.Parameter(torch.empty(0))

        self.embedding_method = config.embedding_method
        dataset_properties = config.dataset_properties

        # if load from mlm pretrained model
        if hasattr(config, "pretrained_dir") and config.pretrained_dir:
            if not os.path.exists(config.pretrained_dir):
                raise ValueError(f"pretrained dir {config.pretrained_dir} not exist")

            print(f"load from pretrained model from {config.pretrained_dir}")
            model_config_path = os.path.join(config.pretrained_dir, "config.json")
            model_config = AutoConfig.from_pretrained(model_config_path)
            model_config.__dict__["hidden_dropout_prob"] = config.hidden_dropout_prob
            model_config.__dict__["attention_probs_dropout_prob"] = config.attention_probs_dropout_prob
            self.backbone = AutoModelForMaskedLM.from_config(model_config)
            pretrained_model_path = os.path.join(config.pretrained_dir, "pytorch_model.bin")
            pretrained_model = torch.load(pretrained_model_path)
            self.backbone.load_state_dict(pretrained_model)
        else:
            model_config = AutoConfig.from_pretrained(config.model_name)
            model_config.__dict__["hidden_dropout_prob"] = config.hidden_dropout_prob
            model_config.__dict__["attention_probs_dropout_prob"] = config.attention_probs_dropout_prob
            self.backbone = AutoModelForMaskedLM.from_pretrained(config.model_name, config=model_config)
        
        # if reinitialize the last few layers
        if hasattr(config, "reinit_layer"):
            if config.reinit_layer < 0:
                raise ValueError(f"reinit_layer should be > 0. current: {config.reinit_layer}")
            
            print(f'Reinitializing last {config.reinit_layer} Layers')
            model_type = config.model_name.split("-")[0]
            encoder_temp = getattr(self.backbone, model_type)
            for layer in encoder_temp.encoder.layer[-config.reinit_layer:]:
                for module in layer.modules():
                    if isinstance(module, nn.Linear):
                        module.weight.data.normal_(mean=0.0, std=model_config.initializer_range)
                        if module.bias is not None:
                            module.bias.data.zero_()
                    elif isinstance(module, nn.Embedding):
                        module.weight.data.normal_(mean=0.0, std=model_config.initializer_range)
                        if module.padding_idx is not None:
                            module.weight.data[module.padding_idx].zero_()
                    elif isinstance(module, nn.LayerNorm):
                        module.bias.data.zero_()
                        module.weight.data.fill_(1.0)

        last_layers = create_last_layers(dataset_properties, model_config.hidden_size)
        self.last_layers = nn.ModuleDict({name: layer for name, layer in last_layers.items()})

        if self.embedding_method == 'weight-pool':
            self.output_emb_layer = WeightedLayerPooling(
                model_config.num_hidden_layers,
                layer_start=config.embedding_layer_start,
                layer_weights=None)
    
    def _get_last_embedding(self, token_ids, **kwargs):
        attention_mask = kwargs.get('attention_mask', None)
        output_dict = self.backbone(token_ids, 
            attention_mask=attention_mask, 
            output_hidden_states=True, 
            return_dict=True)
        hidden_states = output_dict['hidden_states']
        if self.embedding_method == 'last-avg':
            last_hidden_states = hidden_states[-1]
            avg_last_hidden_states = torch.mean(last_hidden_states, dim=1)
            return avg_last_hidden_states
        elif self.embedding_method == 'weight-pool':
            last_emb = self.output_emb_layer(hidden_states)
            return last_emb
        else:
            raise ValueError(f"Unsupported embedding_method {self.embedding_method}.")


    def forward(self, collate_batch, **kwargs):
        """
        Input
        collate_batch: dict. generated by correspoding Collator.
            assume key '{dataset_name}_token_ids' exists.
        Ouptut
        output: dict.
        loss: Optional[torch.Tensor]. size: [1] 
        """
        dataset_property = kwargs.get('dataset_property')
        branches = dataset_property['branches']

        token_ids = None
        for key in collate_batch.keys():
            if key.find("token_ids") != -1:
                dataset_name = key.split("_")[0]
                token_ids = collate_batch[key]
        if token_ids is None:
            raise ValueError("collate_batch should contain key '\{dataset_name\}_token_ids'")

        output_dict = dict()
        last_embedding = self._get_last_embedding(token_ids, **collate_batch)
        for branch in branches:
            try:
                key = "_".join([dataset_name, branch])
                layer = self.last_layers[key]
                output_dict[key] = layer(last_embedding)
            except Exception:
                raise ValueError(f"{key} last layer doesn't exist. Please check your Collator implementation.")
        
        
        loss = None
        if self.training:
            self.loss_wrapper = LossWrapper(dataset_property['loss_name'])
            loss = self.loss_wrapper.forward(collate_batch, output_dict)

        return output_dict, loss
    
    def predict(self, collate_batch, dataset_property):
        """
        Input
        collate_batch: dict. generated by correspoding Collator.
            assume key '{dataset_name}_token_ids' exists.
        Ouptut
        output: torch.Tensor. size [batch_size]
        """
        output_dict, _ = self.forward(collate_batch, dataset_property=dataset_property)
        task = dataset_property["task"]
        label_name = "_".join([dataset_property['name'], dataset_property['branches'][0]])
        output = output_dict[label_name]
        if task == 'reg':
            return output
        elif task == 'cls':
            class_to_score = get_class_to_score(
                dataset_property['range_min'],
                dataset_property['range_max'],
                dataset_property['interval']
            ).to(self.dummy_param.device)
            mean = prob_to_mean(output, class_to_score)
            return mean
        else:
            raise ValueError(f"Unknown task {task}. Should be in (cls/reg)")
