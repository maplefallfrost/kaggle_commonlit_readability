import torch
import torch.nn as nn
from torch.utils import data

from models.roberta import Roberta
from loss import Gaussian_js_loss
from util import extract_dataset_name, to_device
from util import get_class_to_score, prob_to_mean

# replica of constant.py for dealing with circular import
model_name_to_model = {
    "roberta-base": Roberta
}

class SelfDistill(nn.Module):
    def __init__(self, config):
        """
        config: argparse.Namespace.
        """
        super().__init__()

        """
        Model 1 loads from pretrained and model 2 doesn't load from pretrained to increase diversity.
        so it needs to provide mlm pretrained model.
        """
        if not hasattr(config, "pretrained_dir"):
            assert ValueError("SelfDistill model assumes pretrained model exists.")

        self.model_1 = model_name_to_model[config.model_name](config)
        self.model_1 = self.model_1.to("cuda:0")

        # don't change the config
        config.pretrained_dir, tmp = None, config.pretrained_dir
        self.model_2 = model_name_to_model[config.model_name](config)
        config.pretrained_dir = tmp
        self.model_2 = self.model_2.to("cuda:1")
    
    def forward(self, collatch_batch, **kwargs):
        """
        Input
        collate_batch: dict. generated by correspoding Collator.
            assume key '{dataset_name}_token_ids' exists.
        Ouptut
        output: dict.
        loss: Optional[torch.Tensor]. size: [1] 
        """
        model_1_input = to_device(collatch_batch, "cuda:0")
        model_2_input = to_device(collatch_batch, "cuda:1")
        model_1_output_dict, model_1_loss = self.model_1(model_1_input, **kwargs)
        model_2_output_dict, model_2_loss = self.model_2(model_2_input, **kwargs)
        model_2_output_dict = to_device(model_2_output_dict, "cuda:0")

        dataset_property = kwargs['dataset_property']
        loss = None
        if self.training:
            model_2_loss = model_2_loss.to("cuda:0")
            output_gt_loss = torch.mean(torch.stack([model_1_loss, model_2_loss]))
            output_output_loss = 0
            # if dataset_property["task"] == 'reg':
            #     # for simplicity, when the task is 'reg', assume model_1 and model_2 both output mean and std.
            #     # use JS divergence with Gaussian distribution to evaluate the difference between model_1 and model_2's output
            #     # not to write general codes
            #     dataset_name = extract_dataset_name(model_1_output_dict)
            #     label_name = "_".join([dataset_name, "label"])
            #     standard_error_name = "_".join([dataset_name, "standard_error"])
            #     output_output_loss = Gaussian_js_loss(
            #         model_1_output_dict[label_name], model_1_output_dict[standard_error_name],
            #         model_2_output_dict[label_name], model_2_output_dict[standard_error_name]
            #     )
            #     epoch = kwargs.get('epoch', 0)
            #     if epoch < dataset_property['distill_start_epoch']:
            #         output_output_loss = 0
            # else:
            #     output_output_loss = 0
            loss = output_gt_loss + output_output_loss

        output_dict = self._merge_output_dict(model_1_output_dict, model_2_output_dict)
        return output_dict, loss
    
    def _merge_output_dict(self, model_1_output_dict, model_2_output_dict):
        output_dict = dict()
        for key in model_1_output_dict:
            output_dict[key] = 0.5 * (model_1_output_dict[key] + model_2_output_dict[key])
        return output_dict
    
    def predict(self, collate_batch, dataset_property, **kwargs):
        """
        Input
        collate_batch: dict. generated by correspoding Collator.
            assume key '{dataset_name}_token_ids' exists.
        Ouptut
        output: torch.Tensor. size [batch_size]
        """
        output_dict, _ = self.forward(collate_batch, dataset_property=dataset_property)
        label_name = "_".join([dataset_property['name'], 'label'])
        task = dataset_property["task"]
        if dataset_property['predict_method'] == 'knn':
            if 'knn_helper' not in kwargs:
                raise ValueError("predict method: knn require 'knn_helper' parameter in kwargs")
            knn_helper = kwargs['knn_helper']
            pred_emb = output_dict["_".join([dataset_property['name'], 'last_emb'])].cpu().numpy()
            pred = knn_helper.predict(pred_emb)
            return pred

        output = output_dict[label_name]
        if task == 'reg':
            return output.cpu().numpy()
        elif task == 'cls':
            class_to_score = get_class_to_score(
                dataset_property['range_min'],
                dataset_property['range_max'],
                dataset_property['interval']
            ).to(self.dummy_param.device)
            mean = prob_to_mean(output, class_to_score)
            return mean.cpu().numpy()
        else:
            raise ValueError(f"Unknown task {task}. Should be in (cls/reg)")
