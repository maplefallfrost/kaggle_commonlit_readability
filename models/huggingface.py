import torch
import torch.nn as nn

from models.base import BaseModel
from loss import LossWrapper

class HuggingFaceBaseModel(BaseModel):
    def __init__(self):
        super().__init__()

        # just for getting the model device.
        self.dummy_param = nn.Parameter(torch.empty(0))

    def _get_last_embedding(self, token_ids, **kwargs):
        attention_mask = kwargs.get('attention_mask', None)
        output_dict = self.backbone(token_ids, 
            attention_mask=attention_mask, 
            output_hidden_states=True, 
            return_dict=True)
        hidden_states = output_dict['hidden_states']
        if self.embedding_method == 'last-avg':
            last_hidden_states = hidden_states[-1]
            avg_last_hidden_states = torch.mean(last_hidden_states, dim=1)
            return avg_last_hidden_states
        elif self.embedding_method == 'weight-pool':
            last_emb = self.output_emb_layer(hidden_states)
            return last_emb
        else:
            raise ValueError(f"Unsupported embedding_method {self.embedding_method}.")
    
    def forward(self, collate_batch, dataset_property, **kwargs):
        """
        Input
        collate_batch: dict. generated by correspoding Collator.
            assume key '{dataset_name}_token_ids' exists.
        Ouptut
        output: dict.
        loss: Optional[torch.Tensor]. size: [1] 
        """
        branches = dataset_property['branches']

        token_ids = None
        for key in collate_batch.keys():
            if key.find("token_ids") != -1:
                dataset_name = key.split("_")[0]
                token_ids = collate_batch[key]
        if token_ids is None:
            raise ValueError("collate_batch should contain key '\{dataset_name\}_token_ids'")

        output_dict = dict()
        last_embedding = self._get_last_embedding(token_ids, **collate_batch)
        output_dict["_".join([dataset_name, 'last_emb'])] = last_embedding
        for branch in branches:
            try:
                key = "_".join([dataset_name, branch])
                layer = self.last_layers[key]
                output_dict[key] = layer(last_embedding)
            except Exception:
                raise ValueError(f"{key} last layer doesn't exist. Please check your Collator implementation.")
        
        loss = None
        if self.training:
            self.loss_wrapper = LossWrapper(dataset_property['loss_name'])
            loss = self.loss_wrapper.forward(collate_batch, output_dict)

        return output_dict, loss
